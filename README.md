# AI Speech Enhancement - Clean Voice

A GPU-accelerated deep learning system for speech enhancement using U-Net architecture.

## ğŸ¯ Project Overview

This project implements a complete pipeline for removing background noise from speech recordings using a U-Net model trained on the VoiceBank+DEMAND dataset.

## ğŸ“ Project Structure

```
ai-clrvoice/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model.py              # U-Net model implementation
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ gpu.ipynb             # Working GPU training notebook
â”‚   â””â”€â”€ 07_gpu_training.ipynb # Complete GPU training pipeline
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py               # Web interface for audio enhancement
â”‚   â””â”€â”€ requirements.txt     # Streamlit dependencies
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_gpu_model.pth   # Trained model checkpoint
â”œâ”€â”€ dataset/                 # VoiceBank+DEMAND dataset
â”œâ”€â”€ results/                 # Training results and enhanced audio
â”œâ”€â”€ inference_gpu.py         # Standalone inference script
â”œâ”€â”€ test_quality.py         # Quality assessment tools
â””â”€â”€ requirements.txt        # Project dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Use the Web Interface
```bash
cd streamlit_app
streamlit run app.py
```

### 3. Command Line Inference
```bash
python inference_gpu.py input_noisy.wav output_enhanced.wav
```

## ğŸ§  Model Architecture

- **U-Net Architecture**: Encoder-decoder with skip connections
- **Parameters**: 1,927,841 trainable parameters
- **Input**: Magnitude spectrograms (513 x 128)
- **Output**: Enhanced magnitude spectrograms
- **Training**: GPU-accelerated with chunked data loading

### Model Configuration:
- Base filters: 32
- Depth: 3 layers
- Dropout: 0.1
- Sample rate: 16kHz
- STFT: n_fft=1024, hop_length=256

## ğŸ“Š Performance

- **Validation Loss**: 0.000071
- **Average Improvement**: 56.3%
- **SNR Improvement**: +5.84 dB on test samples
- **Processing**: Real-time capable on GPU

## ğŸ› ï¸ Training

To retrain the model, use the `gpu.ipynb` notebook:

1. Open `notebooks/gpu.ipynb`
2. Run all cells in sequence
3. Model will be saved to `models/best_gpu_model.pth`

### Training Features:
- Chunked data loading for memory efficiency
- GPU optimization with CUDA
- Early stopping and checkpointing
- Robust error handling and recovery

## ğŸ“ˆ Usage Examples

### Web Interface
1. Upload a noisy audio file
2. Click "Enhance Audio"
3. Listen to results and download enhanced audio
4. View waveform and spectrogram comparisons

### Command Line
```bash
# Enhance a single file
python inference_gpu.py noisy_speech.wav clean_speech.wav

# Quality assessment
python test_quality.py
```

## ğŸµ Supported Audio Formats

- WAV (recommended)
- MP3
- FLAC
- M4A

## ğŸ”§ Technical Details

### Data Processing:
- 4-second chunks with 25% overlap
- STFT-based magnitude processing
- Min-max normalization
- Crossfading for chunk blending

### GPU Optimization:
- CUDA memory management
- Batch processing
- Gradient clipping
- Mixed precision support

## ğŸ“ Requirements

- Python 3.8+
- PyTorch 1.9+
- CUDA-capable GPU (recommended)
- 4GB+ GPU memory for training
- 16GB+ RAM for dataset loading

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- VoiceBank+DEMAND dataset for training data
- U-Net architecture for speech enhancement
- PyTorch team for the deep learning framework
- Streamlit for the web interface framework
